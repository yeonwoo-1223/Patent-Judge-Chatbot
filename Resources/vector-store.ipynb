{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 여러 PDF 처리\n",
    "files = [\n",
    "    \"특허_실용신안 심사기준.pdf\"\n",
    "]\n",
    "\n",
    "all_docs = []\n",
    "for file in files:\n",
    "    loader = PyPDFLoader(file)\n",
    "    docs = loader.load()\n",
    "    all_docs.extend(docs)\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(all_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "from langchain.schema import Document\n",
    "\n",
    "# PDF에서 텍스트 추출\n",
    "docs = []\n",
    "\n",
    "# 폴더 내 파일 가져오기\n",
    "path = 'pdf_files/'\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "for doc_num, file_name in enumerate(file_list):\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    try:\n",
    "        extracted_text = \"\"  # PDF 한 파일의 전체 텍스트를 저장할 변수\n",
    "        with pdfplumber.open(file_path) as pdf_file:\n",
    "            for i, page in enumerate(pdf_file.pages):\n",
    "                try:\n",
    "                    # 텍스트 추출\n",
    "                    text = page.extract_text()\n",
    "                    if text:\n",
    "                        extracted_text += text + \"\\n\"  # 각 페이지의 텍스트를 합침\n",
    "                except Exception as e:\n",
    "                    print(f\"페이지 {i + 1}에서 오류 발생: {e}\")\n",
    "\n",
    "        # 한 PDF 파일의 전체 텍스트를 하나의 Document로 저장\n",
    "        if extracted_text.strip():  # 추출된 텍스트가 있으면 저장\n",
    "            document = Document(\n",
    "                page_content=extracted_text,\n",
    "                metadata={\"doc_number\": doc_num + 1, \"file_name\": file_name}\n",
    "            )\n",
    "            docs.append(document)\n",
    "        else:\n",
    "            print(f\"파일 '{file_name}'에서 텍스트 추출이 없습니다.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"파일 '{file_name}'에서 오류 발생: {e}\")\n",
    "\n",
    "# 텍스트 추출 결과 확인\n",
    "print(f\"총 {len(docs)}개의 PDF 파일에서 텍스트를 추출했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지로 처리되어 내용이 없는 파일 삭제\n",
    "print(docs[0].page_content)\n",
    "\n",
    "docs = docs[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 과실 비율 데이터 읽어오기\n",
    "# 폴더 내에 있는 파일 모두 가져오기\n",
    "with open('patent_cases_all_pages.json', 'r', encoding='utf-8') as f:\n",
    "    file = json.load(f)\n",
    "\n",
    "\n",
    "# JSON 데이터를 Document로 변환\n",
    "def nested_json_to_documents(json_data):\n",
    "    docs = []  # 문서 리스트 초기화\n",
    "    # 중첩된 리스트를 순회하며 평탄화\n",
    "    for entry in json_data:  # 최상위 리스트 순회\n",
    "        content = (\n",
    "            f\"사건 제목: {entry['제목']}\\n\"\n",
    "            f\"상세 내용 확인 링크: {entry['링크']}\\n\"\n",
    "        )\n",
    "        docs.append(Document(page_content=content))  # Document 객체 추가\n",
    "    return docs\n",
    "\n",
    "docs_rate = nested_json_to_documents(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import Document\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 문서 불러오기\n",
    "with open(\"patent_cases_all_pages.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "docs = []\n",
    "for entry in json_data:\n",
    "    title = entry.get(\"제목\", \"\")\n",
    "    link = entry.get(\"링크\", \"\")\n",
    "    content = f\"제목: {title}\\n링크: {link}\"\n",
    "    metadata = {k: v for k, v in entry.items()}\n",
    "    docs.append(Document(page_content=content, metadata=metadata))\n",
    "\n",
    "# GPT 모델 설정\n",
    "model = ChatOpenAI(model_name=\"gpt-4o\")\n",
    "\n",
    "# 프롬프트 설정\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', '주어진 문서의 \"제목\"과 \"링크\"만 보고 사건의 성격을 한 문장으로 요약해줘. 마치 뉴스 기사처럼 요약해줘. 예: \"이 사건은 A와 B 간의 어떠한 특허권 분쟁으로, 어떤 법적 책임 여부가 쟁점이 되었으며 판결은 어떻게 되었다\"'),\n",
    "    ('user', '{content}')\n",
    "])\n",
    "\n",
    "# 요약 함수\n",
    "def summarize_accident(accident_text):\n",
    "    try:\n",
    "        messages = summary_prompt.format_messages(content=accident_text)\n",
    "        response = model(messages)\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        print(f\"요약 중 오류 발생: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 요약 문서 생성 함수\n",
    "def summary_docs(original_doc, summary_text):\n",
    "    title = original_doc.metadata.get(\"제목\", \"\")\n",
    "    link = original_doc.metadata.get(\"링크\", \"\")\n",
    "    combined_text = f\"제목: {title}\\n링크: {link}\\n요약: {summary_text}\"\n",
    "    return Document(\n",
    "        page_content=combined_text,\n",
    "        metadata={**original_doc.metadata, 'summary': summary_text}\n",
    "    )\n",
    "\n",
    "# 캐시 파일 경로\n",
    "CACHE_PATH = \"summaries_cache.json\"\n",
    "\n",
    "# 캐시 불러오기\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    with open(CACHE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        cached_summaries = json.load(f)\n",
    "else:\n",
    "    cached_summaries = {}\n",
    "\n",
    "# 요약 실행\n",
    "summarized_docs = []\n",
    "total = len(docs)\n",
    "\n",
    "for i, doc in enumerate(tqdm(docs, desc=\"요약 중\", unit=\"문서\", ncols=100), 1):\n",
    "    title = doc.metadata.get(\"제목\", \"\")\n",
    "    print(f\"[{i}/{total}] 처리 중: {title}\")\n",
    "\n",
    "    if title in cached_summaries:\n",
    "        summary = cached_summaries[title]\n",
    "    else:\n",
    "        summary = summarize_accident(doc.page_content)\n",
    "        cached_summaries[title] = summary\n",
    "\n",
    "    summarized_docs.append(summary_docs(doc, summary))\n",
    "\n",
    "# 캐시 저장\n",
    "with open(CACHE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cached_summaries, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n요약 완료 ✅ 총 문서 수: {len(summarized_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from uuid import uuid4\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "# 1. OpenAI 임베딩 모델\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# 2. Document 로딩 (예: 요약문 기준)\n",
    "with open(\"summaries_cache.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary_data = json.load(f)\n",
    "\n",
    "summarized_docs = [\n",
    "    Document(page_content=summary, metadata={\"title\": title})\n",
    "    for title, summary in summary_data.items()\n",
    "]\n",
    "\n",
    "# 3. UUID 생성\n",
    "uuids1 = [f\"docs_{i+1}\" for i in range(len(summarized_docs))]\n",
    "\n",
    "# 4. FAISS 벡터 저장 함수\n",
    "def build_vectorstore_batched(docs: List[Document], ids: List[str], batch_size=100) -> FAISS:\n",
    "    stores = []\n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        batch_docs = docs[i:i + batch_size]\n",
    "        batch_ids = ids[i:i + batch_size]\n",
    "        store = FAISS.from_documents(documents=batch_docs, ids=batch_ids, embedding=embeddings)\n",
    "        stores.append(store)\n",
    "\n",
    "    base_store = stores[0]\n",
    "    for store in stores[1:]:\n",
    "        base_store.merge_from(store)\n",
    "    return base_store\n",
    "\n",
    "# 5. 벡터 생성 및 저장\n",
    "vector_store_law = build_vectorstore_batched(summarized_docs, uuids1)\n",
    "vector_store_law.save_local(\"vector_store_law\", index_name=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "# 1. OpenAI 임베딩 모델 초기화\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# 2. FAISS 벡터 저장 함수 정의\n",
    "def build_vectorstore_batched(docs: List[Document], ids: List[str], batch_size=100) -> FAISS:\n",
    "    if not docs:\n",
    "        raise ValueError(\"❌ 입력된 문서 리스트가 비어 있습니다.\")\n",
    "    stores = []\n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        batch_docs = docs[i:i + batch_size]\n",
    "        batch_ids = ids[i:i + batch_size]\n",
    "        store = FAISS.from_documents(documents=batch_docs, ids=batch_ids, embedding=embeddings)\n",
    "        stores.append(store)\n",
    "    base_store = stores[0]\n",
    "    for store in stores[1:]:\n",
    "        base_store.merge_from(store)\n",
    "    return base_store\n",
    "\n",
    "# 3. summaries_cache.json 로딩\n",
    "with open(\"summaries_cache.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary_data = json.load(f)\n",
    "\n",
    "# 4. 문서 분리\n",
    "docs_law = []\n",
    "docs_situation = []\n",
    "docs_rate = []\n",
    "\n",
    "for title, summary in summary_data.items():\n",
    "    doc = Document(page_content=summary, metadata={\"title\": title})\n",
    "    if \"침해\" in summary or \"등록무효\" in summary:\n",
    "        docs_law.append(doc)\n",
    "    elif \"사실관계\" in summary or \"상황\" in summary:\n",
    "        docs_situation.append(doc)\n",
    "    elif \"보상\" in summary or \"배상\" in summary:\n",
    "        docs_rate.append(doc)\n",
    "\n",
    "# 5. UUID 생성\n",
    "uuids_law = [f\"law_{i+1}\" for i in range(len(docs_law))]\n",
    "uuids_situation = [f\"situation_{i+1}\" for i in range(len(docs_situation))]\n",
    "uuids_rate = [f\"rate_{i+1}\" for i in range(len(docs_rate))]\n",
    "\n",
    "# 6. 벡터 저장\n",
    "if docs_law:\n",
    "    vector_store_law = build_vectorstore_batched(docs_law, uuids_law)\n",
    "    vector_store_law.save_local(\"vector_store_law\", index_name=\"index\")\n",
    "    print(\"✅ vector_store_law 저장 완료\")\n",
    "\n",
    "if docs_situation:\n",
    "    vector_store_situation = build_vectorstore_batched(docs_situation, uuids_situation)\n",
    "    vector_store_situation.save_local(\"vector_store_situation\", index_name=\"index\")\n",
    "    print(\"✅ vector_store_situation 저장 완료\")\n",
    "\n",
    "if docs_rate:\n",
    "    vector_store_rate = build_vectorstore_batched(docs_rate, uuids_rate)\n",
    "    vector_store_rate.save_local(\"vector_store_rate\", index_name=\"index\")\n",
    "    print(\"✅ vector_store_rate 저장 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불러오기\n",
    "vector_store_law = FAISS.load_local(\n",
    "    \"vector_store_law\",\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True,\n",
    "    index_name=\"index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 결과 1: [상표]솔표 사건(특허법원 2023허29)\n",
      "이 사건은 솔표 상표를 둘러싼 분쟁으로, 상표권 침해 여부가 쟁점이 되었으며 특허법원에서 판결이 이루어졌다.\n",
      "\n",
      "🔹 결과 2: [상표]조선협객전 사건(특허법원 2023허12916)\n",
      "이 사건은 \"조선협객전\"이라는 상표와 관련된 분쟁으로, 상표권 침해 여부가 쟁점이 되었으며 특허법원에서 판결이 이루어졌다.\n",
      "\n",
      "🔹 결과 3: [민사]소위 명품 리폼 영업을 하면서 원고의 상표를 그대로 표시한 사건에서, 피고는 상표권침해금지 및 손해배상 의무가 있다고 판단한 예(특허법원 2023나11283)\n",
      "이 사건은 명품 리폼 영업을 하면서 원고의 상표를 무단으로 사용한 피고가 상표권 침해 및 손해배상 책임이 있는지에 대한 분쟁으로, 법원은 피고에게 상표권침해금지 및 손해배상 의무가 있다고 판결하였다.\n"
     ]
    }
   ],
   "source": [
    "#저장 잘 되었는지 확인용\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 1. 임베딩 모델 로드\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# 2. 벡터 스토어 불러오기 (침해 관련 사건 벡터 저장소)\n",
    "vector_store_law = FAISS.load_local(\n",
    "    \"vector_store_law\",              # 저장된 폴더 경로\n",
    "    embeddings=embedding_model,\n",
    "    index_name=\"index\",              # 저장 당시 사용한 index_name\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# 3. 검색 쿼리 작성\n",
    "query = \"상표권 침해와 관련된 판례 알려줘\"\n",
    "\n",
    "# 4. 유사도 검색 실행\n",
    "results = vector_store_law.similarity_search(query, k=3)\n",
    "\n",
    "# 5. 결과 출력\n",
    "for i, res in enumerate(results, 1):\n",
    "    title = res.metadata.get(\"title\", \"[제목 없음]\")\n",
    "    print(f\"\\n🔹 결과 {i}: {title}\")\n",
    "    print(res.page_content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
