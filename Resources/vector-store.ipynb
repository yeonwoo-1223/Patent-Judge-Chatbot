{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "#모델 설정\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 파일 로드. 파일의 경로 입력\n",
    "loader = PyPDFLoader(\"231107_과실비율인정기준_온라인용.pdf\")\n",
    "\n",
    "# 페이지 별 문서 로드\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 '2013가합15831_판결문_검수완료.pdf'에서 텍스트 추출이 없습니다.\n",
      "총 125개의 PDF 파일에서 텍스트를 추출했습니다.\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "from langchain.schema import Document\n",
    "\n",
    "# PDF에서 텍스트 추출\n",
    "docs = []\n",
    "\n",
    "# 폴더 내 파일 가져오기\n",
    "path = 'pdf_files/'\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "for doc_num, file_name in enumerate(file_list):\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    try:\n",
    "        extracted_text = \"\"  # PDF 한 파일의 전체 텍스트를 저장할 변수\n",
    "        with pdfplumber.open(file_path) as pdf_file:\n",
    "            for i, page in enumerate(pdf_file.pages):\n",
    "                try:\n",
    "                    # 텍스트 추출\n",
    "                    text = page.extract_text()\n",
    "                    if text:\n",
    "                        extracted_text += text + \"\\n\"  # 각 페이지의 텍스트를 합침\n",
    "                except Exception as e:\n",
    "                    print(f\"페이지 {i + 1}에서 오류 발생: {e}\")\n",
    "\n",
    "        # 한 PDF 파일의 전체 텍스트를 하나의 Document로 저장\n",
    "        if extracted_text.strip():  # 추출된 텍스트가 있으면 저장\n",
    "            document = Document(\n",
    "                page_content=extracted_text,\n",
    "                metadata={\"doc_number\": doc_num + 1, \"file_name\": file_name}\n",
    "            )\n",
    "            docs.append(document)\n",
    "        else:\n",
    "            print(f\"파일 '{file_name}'에서 텍스트 추출이 없습니다.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"파일 '{file_name}'에서 오류 발생: {e}\")\n",
    "\n",
    "# 텍스트 추출 결과 확인\n",
    "print(f\"총 {len(docs)}개의 PDF 파일에서 텍스트를 추출했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "본 판결문은 판결서 인터넷열람 사이트에서 열람·출력되었습니다.\n",
      "영리목적으로 이용하거나 무단 배포를 금합니다.게시일자 : 2016-10-07\n",
      "본 판결문은 판결서 인터넷열람 사이트에서 열람·출력되었습니다.\n",
      "영리목적으로 이용하거나 무단 배포를 금합니다.게시일자 : 2016-10-07\n",
      "본 판결문은 판결서 인터넷열람 사이트에서 열람·출력되었습니다.\n",
      "영리목적으로 이용하거나 무단 배포를 금합니다.게시일자 : 2016-10-07\n",
      "본 판결문은 판결서 인터넷열람 사이트에서 열람·출력되었습니다.\n",
      "영리목적으로 이용하거나 무단 배포를 금합니다.게시일자 : 2016-10-07\n",
      "본 판결문은 판결서 인터넷열람 사이트에서 열람·출력되었습니다.\n",
      "영리목적으로 이용하거나 무단 배포를 금합니다.게시일자 : 2016-10-07\n",
      "본 판결문은 판결서 인터넷열람 사이트에서 열람·출력되었습니다.\n",
      "영리목적으로 이용하거나 무단 배포를 금합니다.게시일자 : 2016-10-07\n",
      "본 판결문은 판결서 인터넷열람 사이트에서 열람·출력되었습니다.\n",
      "영리목적으로 이용하거나 무단 배포를 금합니다.게시일자 : 2016-10-07\n",
      "본 판결문은 판결서 인터넷열람 사이트에서 열람·출력되었습니다.\n",
      "영리목적으로 이용하거나 무단 배포를 금합니다.게시일자 : 2016-10-07\n",
      "본 판결문은 판결서 인터넷열람 사이트에서 열람·출력되었습니다.\n",
      "영리목적으로 이용하거나 무단 배포를 금합니다.게시일자 : 2016-10-07\n",
      "본 판결문은 판결서 인터넷열람 사이트에서 열람·출력되었습니다.\n",
      "영리목적으로 이용하거나 무단 배포를 금합니다.게시일자 : 2016-10-07\n",
      "본 판결문은 판결서 인터넷열람 사이트에서 열람·출력되었습니다.\n",
      "영리목적으로 이용하거나 무단 배포를 금합니다.게시일자 : 2016-10-07\n",
      "본 판결문은 판결서 인터넷열람 사이트에서 열람·출력되었습니다.\n",
      "영리목적으로 이용하거나 무단 배포를 금합니다.게시일자 : 2016-10-07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 이미지로 처리되어 내용이 없는 파일 삭제\n",
    "print(docs[0].page_content)\n",
    "\n",
    "docs = docs[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 과실 비율 데이터 읽어오기\n",
    "# 폴더 내에 있는 파일 모두 가져오기\n",
    "with open('accident_data_all_pages.json', 'r', encoding='utf-8') as f:\n",
    "    file = json.load(f)\n",
    "\n",
    "\n",
    "# JSON 데이터를 Document로 변환\n",
    "def nested_json_to_documents(json_data):\n",
    "    docs = []  # 문서 리스트 초기화\n",
    "    # 중첩된 리스트를 순회하며 평탄화\n",
    "    for entry in json_data:  # 최상위 리스트 순회\n",
    "        content = (\n",
    "            f\"상황: {entry['상황']}\\n\"\n",
    "            f\"청구인 과실 비율: {entry['청구인 과실 비율']}\\n\"\n",
    "            f\"피청구인 과실 비율: {entry['피청구인 과실 비율']}\\n\"\n",
    "        )\n",
    "        docs.append(Document(page_content=content))  # Document 객체 추가\n",
    "    return docs\n",
    "\n",
    "docs_rate = nested_json_to_documents(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 사고 상황 요약을 위한 프롬프트 템플릿 정의\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', '주어진 문서 내의 \"교통사고 발생 상황\"을 \"사고 원인\"을 포함해서 한 문장으로 요약해줘. 단순 사고 상황에 대한 내용만 다루고 사건에 대한 판결의 내용은 넣지 말아줘.'),\n",
    "    ('user', '{content}')\n",
    "])\n",
    "\n",
    "# 사고 상황을 요약하는 함수 (LLM 모델 사용)\n",
    "def summarize_accident(accident_text):\n",
    "    summary = summary_prompt.format_messages(content=accident_text)\n",
    "    result = model.invoke(summary)\n",
    "    return result  # 요약된 사고 상황 반환\n",
    "\n",
    "\n",
    "# 문서 요약본 새 document로 저장\n",
    "def summary_docs(original_doc, summary_text):\n",
    "    updated_doc = Document(\n",
    "        metadata={**original_doc.metadata, 'summary': summary_text},  # 요약 추가\n",
    "        page_content=original_doc.page_content,\n",
    "    )\n",
    "    return updated_doc\n",
    "\n",
    "\n",
    "# 요약된 문서들을 저장할 리스트\n",
    "summarized_docs = []\n",
    "\n",
    "# 모든 문서에 대해 요약 생성, 저장 및 임베딩\n",
    "for i in range(len(docs)):\n",
    "    summary = summarize_accident(docs[i].page_content)  # 요약 생성\n",
    "    sum_doc = summary_docs(docs[i], summary)  # 요약 추가\n",
    "    summarized_docs.append(sum_doc)  # 리스트에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from uuid import uuid4\n",
    "\n",
    "# OpenAI 임베딩 모델 초기화\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# 문서 별 고유 ID 생성\n",
    "uuids = [f\"page_{i+1}\" for i in range(len(splits))]\n",
    "uuids1 = [f\"docs_{i+1}\" for i in range(len(summarized_docs))]\n",
    "uuids2 = [f\"docs_{i+1}\" for i in range(len(docs_rate))]\n",
    "\n",
    "\n",
    "# FAISS 벡터 스토어 생성\n",
    "vector_store_law = FAISS.from_documents(\n",
    "    documents=splits, ids=uuids, embedding=embeddings)\n",
    "vector_store_situation = FAISS.from_documents(\n",
    "    documents=summarized_docs, ids=uuids1, embedding=embeddings)\n",
    "vector_store_rate = FAISS.from_documents(\n",
    "    documents=docs_rate, ids=uuids2, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS 벡터 스토어 저장\n",
    "SAVE_PATH1 = \"vector_store_law\"\n",
    "SAVE_PATH2 = \"vector_store_situation\"\n",
    "SAVE_PATH3 = \"vector_store_rate\"\n",
    "\n",
    "\n",
    "# FAISS 저장\n",
    "vector_store_law.save_local(SAVE_PATH1)\n",
    "vector_store_situation.save_local(SAVE_PATH2)\n",
    "vector_store_rate.save_local(SAVE_PATH3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
