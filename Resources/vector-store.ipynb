{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# ì—¬ëŸ¬ PDF ì²˜ë¦¬\n",
    "files = [\n",
    "    \"íŠ¹í—ˆ_ì‹¤ìš©ì‹ ì•ˆ ì‹¬ì‚¬ê¸°ì¤€.pdf\"\n",
    "]\n",
    "\n",
    "all_docs = []\n",
    "for file in files:\n",
    "    loader = PyPDFLoader(file)\n",
    "    docs = loader.load()\n",
    "    all_docs.extend(docs)\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(all_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "from langchain.schema import Document\n",
    "\n",
    "# PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "docs = []\n",
    "\n",
    "# í´ë” ë‚´ íŒŒì¼ ê°€ì ¸ì˜¤ê¸°\n",
    "path = 'pdf_files/'\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "for doc_num, file_name in enumerate(file_list):\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    try:\n",
    "        extracted_text = \"\"  # PDF í•œ íŒŒì¼ì˜ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•  ë³€ìˆ˜\n",
    "        with pdfplumber.open(file_path) as pdf_file:\n",
    "            for i, page in enumerate(pdf_file.pages):\n",
    "                try:\n",
    "                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "                    text = page.extract_text()\n",
    "                    if text:\n",
    "                        extracted_text += text + \"\\n\"  # ê° í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ í•©ì¹¨\n",
    "                except Exception as e:\n",
    "                    print(f\"í˜ì´ì§€ {i + 1}ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "        # í•œ PDF íŒŒì¼ì˜ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ Documentë¡œ ì €ì¥\n",
    "        if extracted_text.strip():  # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì €ì¥\n",
    "            document = Document(\n",
    "                page_content=extracted_text,\n",
    "                metadata={\"doc_number\": doc_num + 1, \"file_name\": file_name}\n",
    "            )\n",
    "            docs.append(document)\n",
    "        else:\n",
    "            print(f\"íŒŒì¼ '{file_name}'ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"íŒŒì¼ '{file_name}'ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼ í™•ì¸\n",
    "print(f\"ì´ {len(docs)}ê°œì˜ PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ë¯¸ì§€ë¡œ ì²˜ë¦¬ë˜ì–´ ë‚´ìš©ì´ ì—†ëŠ” íŒŒì¼ ì‚­ì œ\n",
    "print(docs[0].page_content)\n",
    "\n",
    "docs = docs[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# ê³¼ì‹¤ ë¹„ìœ¨ ë°ì´í„° ì½ì–´ì˜¤ê¸°\n",
    "# í´ë” ë‚´ì— ìˆëŠ” íŒŒì¼ ëª¨ë‘ ê°€ì ¸ì˜¤ê¸°\n",
    "with open('patent_cases_all_pages.json', 'r', encoding='utf-8') as f:\n",
    "    file = json.load(f)\n",
    "\n",
    "\n",
    "# JSON ë°ì´í„°ë¥¼ Documentë¡œ ë³€í™˜\n",
    "def nested_json_to_documents(json_data):\n",
    "    docs = []  # ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "    # ì¤‘ì²©ëœ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° í‰íƒ„í™”\n",
    "    for entry in json_data:  # ìµœìƒìœ„ ë¦¬ìŠ¤íŠ¸ ìˆœíšŒ\n",
    "        content = (\n",
    "            f\"ì‚¬ê±´ ì œëª©: {entry['ì œëª©']}\\n\"\n",
    "            f\"ìƒì„¸ ë‚´ìš© í™•ì¸ ë§í¬: {entry['ë§í¬']}\\n\"\n",
    "        )\n",
    "        docs.append(Document(page_content=content))  # Document ê°ì²´ ì¶”ê°€\n",
    "    return docs\n",
    "\n",
    "docs_rate = nested_json_to_documents(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import Document\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "with open(\"patent_cases_all_pages.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "docs = []\n",
    "for entry in json_data:\n",
    "    title = entry.get(\"ì œëª©\", \"\")\n",
    "    link = entry.get(\"ë§í¬\", \"\")\n",
    "    content = f\"ì œëª©: {title}\\në§í¬: {link}\"\n",
    "    metadata = {k: v for k, v in entry.items()}\n",
    "    docs.append(Document(page_content=content, metadata=metadata))\n",
    "\n",
    "# GPT ëª¨ë¸ ì„¤ì •\n",
    "model = ChatOpenAI(model_name=\"gpt-4o\")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ì„¤ì •\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'ì£¼ì–´ì§„ ë¬¸ì„œì˜ \"ì œëª©\"ê³¼ \"ë§í¬\"ë§Œ ë³´ê³  ì‚¬ê±´ì˜ ì„±ê²©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜. ë§ˆì¹˜ ë‰´ìŠ¤ ê¸°ì‚¬ì²˜ëŸ¼ ìš”ì•½í•´ì¤˜. ì˜ˆ: \"ì´ ì‚¬ê±´ì€ Aì™€ B ê°„ì˜ ì–´ë– í•œ íŠ¹í—ˆê¶Œ ë¶„ìŸìœ¼ë¡œ, ì–´ë–¤ ë²•ì  ì±…ì„ ì—¬ë¶€ê°€ ìŸì ì´ ë˜ì—ˆìœ¼ë©° íŒê²°ì€ ì–´ë–»ê²Œ ë˜ì—ˆë‹¤\"'),\n",
    "    ('user', '{content}')\n",
    "])\n",
    "\n",
    "# ìš”ì•½ í•¨ìˆ˜\n",
    "def summarize_accident(accident_text):\n",
    "    try:\n",
    "        messages = summary_prompt.format_messages(content=accident_text)\n",
    "        response = model(messages)\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        print(f\"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# ìš”ì•½ ë¬¸ì„œ ìƒì„± í•¨ìˆ˜\n",
    "def summary_docs(original_doc, summary_text):\n",
    "    title = original_doc.metadata.get(\"ì œëª©\", \"\")\n",
    "    link = original_doc.metadata.get(\"ë§í¬\", \"\")\n",
    "    combined_text = f\"ì œëª©: {title}\\në§í¬: {link}\\nìš”ì•½: {summary_text}\"\n",
    "    return Document(\n",
    "        page_content=combined_text,\n",
    "        metadata={**original_doc.metadata, 'summary': summary_text}\n",
    "    )\n",
    "\n",
    "# ìºì‹œ íŒŒì¼ ê²½ë¡œ\n",
    "CACHE_PATH = \"summaries_cache.json\"\n",
    "\n",
    "# ìºì‹œ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    with open(CACHE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        cached_summaries = json.load(f)\n",
    "else:\n",
    "    cached_summaries = {}\n",
    "\n",
    "# ìš”ì•½ ì‹¤í–‰\n",
    "summarized_docs = []\n",
    "total = len(docs)\n",
    "\n",
    "for i, doc in enumerate(tqdm(docs, desc=\"ìš”ì•½ ì¤‘\", unit=\"ë¬¸ì„œ\", ncols=100), 1):\n",
    "    title = doc.metadata.get(\"ì œëª©\", \"\")\n",
    "    print(f\"[{i}/{total}] ì²˜ë¦¬ ì¤‘: {title}\")\n",
    "\n",
    "    if title in cached_summaries:\n",
    "        summary = cached_summaries[title]\n",
    "    else:\n",
    "        summary = summarize_accident(doc.page_content)\n",
    "        cached_summaries[title] = summary\n",
    "\n",
    "    summarized_docs.append(summary_docs(doc, summary))\n",
    "\n",
    "# ìºì‹œ ì €ì¥\n",
    "with open(CACHE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cached_summaries, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nìš”ì•½ ì™„ë£Œ âœ… ì´ ë¬¸ì„œ ìˆ˜: {len(summarized_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from uuid import uuid4\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "# 1. OpenAI ì„ë² ë”© ëª¨ë¸\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# 2. Document ë¡œë”© (ì˜ˆ: ìš”ì•½ë¬¸ ê¸°ì¤€)\n",
    "with open(\"summaries_cache.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary_data = json.load(f)\n",
    "\n",
    "summarized_docs = [\n",
    "    Document(page_content=summary, metadata={\"title\": title})\n",
    "    for title, summary in summary_data.items()\n",
    "]\n",
    "\n",
    "# 3. UUID ìƒì„±\n",
    "uuids1 = [f\"docs_{i+1}\" for i in range(len(summarized_docs))]\n",
    "\n",
    "# 4. FAISS ë²¡í„° ì €ì¥ í•¨ìˆ˜\n",
    "def build_vectorstore_batched(docs: List[Document], ids: List[str], batch_size=100) -> FAISS:\n",
    "    stores = []\n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        batch_docs = docs[i:i + batch_size]\n",
    "        batch_ids = ids[i:i + batch_size]\n",
    "        store = FAISS.from_documents(documents=batch_docs, ids=batch_ids, embedding=embeddings)\n",
    "        stores.append(store)\n",
    "\n",
    "    base_store = stores[0]\n",
    "    for store in stores[1:]:\n",
    "        base_store.merge_from(store)\n",
    "    return base_store\n",
    "\n",
    "# 5. ë²¡í„° ìƒì„± ë° ì €ì¥\n",
    "vector_store_law = build_vectorstore_batched(summarized_docs, uuids1)\n",
    "vector_store_law.save_local(\"vector_store_law\", index_name=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "# 1. OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# 2. FAISS ë²¡í„° ì €ì¥ í•¨ìˆ˜ ì •ì˜\n",
    "def build_vectorstore_batched(docs: List[Document], ids: List[str], batch_size=100) -> FAISS:\n",
    "    if not docs:\n",
    "        raise ValueError(\"âŒ ì…ë ¥ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    stores = []\n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        batch_docs = docs[i:i + batch_size]\n",
    "        batch_ids = ids[i:i + batch_size]\n",
    "        store = FAISS.from_documents(documents=batch_docs, ids=batch_ids, embedding=embeddings)\n",
    "        stores.append(store)\n",
    "    base_store = stores[0]\n",
    "    for store in stores[1:]:\n",
    "        base_store.merge_from(store)\n",
    "    return base_store\n",
    "\n",
    "# 3. summaries_cache.json ë¡œë”©\n",
    "with open(\"summaries_cache.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary_data = json.load(f)\n",
    "\n",
    "# 4. ë¬¸ì„œ ë¶„ë¦¬\n",
    "docs_law = []\n",
    "docs_situation = []\n",
    "docs_rate = []\n",
    "\n",
    "for title, summary in summary_data.items():\n",
    "    doc = Document(page_content=summary, metadata={\"title\": title})\n",
    "    if \"ì¹¨í•´\" in summary or \"ë“±ë¡ë¬´íš¨\" in summary:\n",
    "        docs_law.append(doc)\n",
    "    elif \"ì‚¬ì‹¤ê´€ê³„\" in summary or \"ìƒí™©\" in summary:\n",
    "        docs_situation.append(doc)\n",
    "    elif \"ë³´ìƒ\" in summary or \"ë°°ìƒ\" in summary:\n",
    "        docs_rate.append(doc)\n",
    "\n",
    "# 5. UUID ìƒì„±\n",
    "uuids_law = [f\"law_{i+1}\" for i in range(len(docs_law))]\n",
    "uuids_situation = [f\"situation_{i+1}\" for i in range(len(docs_situation))]\n",
    "uuids_rate = [f\"rate_{i+1}\" for i in range(len(docs_rate))]\n",
    "\n",
    "# 6. ë²¡í„° ì €ì¥\n",
    "if docs_law:\n",
    "    vector_store_law = build_vectorstore_batched(docs_law, uuids_law)\n",
    "    vector_store_law.save_local(\"vector_store_law\", index_name=\"index\")\n",
    "    print(\"âœ… vector_store_law ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "if docs_situation:\n",
    "    vector_store_situation = build_vectorstore_batched(docs_situation, uuids_situation)\n",
    "    vector_store_situation.save_local(\"vector_store_situation\", index_name=\"index\")\n",
    "    print(\"âœ… vector_store_situation ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "if docs_rate:\n",
    "    vector_store_rate = build_vectorstore_batched(docs_rate, uuids_rate)\n",
    "    vector_store_rate.save_local(\"vector_store_rate\", index_name=\"index\")\n",
    "    print(\"âœ… vector_store_rate ì €ì¥ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "vector_store_law = FAISS.load_local(\n",
    "    \"vector_store_law\",\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True,\n",
    "    index_name=\"index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ ê²°ê³¼ 1: [ìƒí‘œ]ì†”í‘œ ì‚¬ê±´(íŠ¹í—ˆë²•ì› 2023í—ˆ29)\n",
      "ì´ ì‚¬ê±´ì€ ì†”í‘œ ìƒí‘œë¥¼ ë‘˜ëŸ¬ì‹¼ ë¶„ìŸìœ¼ë¡œ, ìƒí‘œê¶Œ ì¹¨í•´ ì—¬ë¶€ê°€ ìŸì ì´ ë˜ì—ˆìœ¼ë©° íŠ¹í—ˆë²•ì›ì—ì„œ íŒê²°ì´ ì´ë£¨ì–´ì¡Œë‹¤.\n",
      "\n",
      "ğŸ”¹ ê²°ê³¼ 2: [ìƒí‘œ]ì¡°ì„ í˜‘ê°ì „ ì‚¬ê±´(íŠ¹í—ˆë²•ì› 2023í—ˆ12916)\n",
      "ì´ ì‚¬ê±´ì€ \"ì¡°ì„ í˜‘ê°ì „\"ì´ë¼ëŠ” ìƒí‘œì™€ ê´€ë ¨ëœ ë¶„ìŸìœ¼ë¡œ, ìƒí‘œê¶Œ ì¹¨í•´ ì—¬ë¶€ê°€ ìŸì ì´ ë˜ì—ˆìœ¼ë©° íŠ¹í—ˆë²•ì›ì—ì„œ íŒê²°ì´ ì´ë£¨ì–´ì¡Œë‹¤.\n",
      "\n",
      "ğŸ”¹ ê²°ê³¼ 3: [ë¯¼ì‚¬]ì†Œìœ„ ëª…í’ˆ ë¦¬í¼ ì˜ì—…ì„ í•˜ë©´ì„œ ì›ê³ ì˜ ìƒí‘œë¥¼ ê·¸ëŒ€ë¡œ í‘œì‹œí•œ ì‚¬ê±´ì—ì„œ, í”¼ê³ ëŠ” ìƒí‘œê¶Œì¹¨í•´ê¸ˆì§€ ë° ì†í•´ë°°ìƒ ì˜ë¬´ê°€ ìˆë‹¤ê³  íŒë‹¨í•œ ì˜ˆ(íŠ¹í—ˆë²•ì› 2023ë‚˜11283)\n",
      "ì´ ì‚¬ê±´ì€ ëª…í’ˆ ë¦¬í¼ ì˜ì—…ì„ í•˜ë©´ì„œ ì›ê³ ì˜ ìƒí‘œë¥¼ ë¬´ë‹¨ìœ¼ë¡œ ì‚¬ìš©í•œ í”¼ê³ ê°€ ìƒí‘œê¶Œ ì¹¨í•´ ë° ì†í•´ë°°ìƒ ì±…ì„ì´ ìˆëŠ”ì§€ì— ëŒ€í•œ ë¶„ìŸìœ¼ë¡œ, ë²•ì›ì€ í”¼ê³ ì—ê²Œ ìƒí‘œê¶Œì¹¨í•´ê¸ˆì§€ ë° ì†í•´ë°°ìƒ ì˜ë¬´ê°€ ìˆë‹¤ê³  íŒê²°í•˜ì˜€ë‹¤.\n"
     ]
    }
   ],
   "source": [
    "#ì €ì¥ ì˜ ë˜ì—ˆëŠ”ì§€ í™•ì¸ìš©\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# 2. ë²¡í„° ìŠ¤í† ì–´ ë¶ˆëŸ¬ì˜¤ê¸° (ì¹¨í•´ ê´€ë ¨ ì‚¬ê±´ ë²¡í„° ì €ì¥ì†Œ)\n",
    "vector_store_law = FAISS.load_local(\n",
    "    \"vector_store_law\",              # ì €ì¥ëœ í´ë” ê²½ë¡œ\n",
    "    embeddings=embedding_model,\n",
    "    index_name=\"index\",              # ì €ì¥ ë‹¹ì‹œ ì‚¬ìš©í•œ index_name\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# 3. ê²€ìƒ‰ ì¿¼ë¦¬ ì‘ì„±\n",
    "query = \"ìƒí‘œê¶Œ ì¹¨í•´ì™€ ê´€ë ¨ëœ íŒë¡€ ì•Œë ¤ì¤˜\"\n",
    "\n",
    "# 4. ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤í–‰\n",
    "results = vector_store_law.similarity_search(query, k=3)\n",
    "\n",
    "# 5. ê²°ê³¼ ì¶œë ¥\n",
    "for i, res in enumerate(results, 1):\n",
    "    title = res.metadata.get(\"title\", \"[ì œëª© ì—†ìŒ]\")\n",
    "    print(f\"\\nğŸ”¹ ê²°ê³¼ {i}: {title}\")\n",
    "    print(res.page_content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
